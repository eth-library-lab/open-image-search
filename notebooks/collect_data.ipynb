{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse-Image Search for Graphische Sammlung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image size options   \n",
    "150x150 default  \n",
    "250x250 resolution=mediumImageResolution  \n",
    "350x350 resolution=highImageResolution  \n",
    "max resolution=superImageResolution  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = https://www.e-gs.ethz.ch/eMP/eMuseumPlus?service=ImageAsset&module=collection&objectId=2562&resolution=mediumImageResolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Downloading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_chromedriver=\"C:\\lib\\chromedriver_win32\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path_to_chromedriver)\n",
    "\n",
    "def document_initialised(driver):\n",
    "    return driver.execute_script(\"return initialised\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_url(soup):\n",
    "    \"\"\"\n",
    "    find link to next page    \n",
    "    \"\"\"\n",
    "\n",
    "#     soup_pagin = soup.find(name=\"ul\", attrs={\"class\":\"pagination\"})\n",
    "    soup_next = soup.find(name=\"a\", attrs={\"class\":\"nextBtn\"})\n",
    "    if soup_next:\n",
    "        next_url = soup_next['href']\n",
    "#         print(next_url)\n",
    "        return next_url\n",
    "    else:\n",
    "        next_url = None\n",
    "    \n",
    "    return next_url\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_list_of_elements(soup):\n",
    "\n",
    "    #find main body col-lg-12 col-md-12 list-row\n",
    "    soup_body = soup.find(name='div', attrs={\"class\":\"col-lg-12 col-md-12 list-row\"})\n",
    "    if soup_body:\n",
    "        soup_el_lst = soup_body.find_all(name='div', attrs={\"class\":\"ssy_galleryElement\"})        \n",
    "    else:\n",
    "        soup_el_lst = []\n",
    "    \n",
    "    return soup_el_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_img_url(soup_el):\n",
    "    \"\"\"\n",
    "    find link to image\n",
    "    accepts the html for a single gallery element\n",
    "    \"\"\"\n",
    "    soup_fig = soup_el.find(name='figure')\n",
    "    if soup_fig:\n",
    "        img_url = soup_fig.find(name='a').find(name='img')['src']\n",
    "#         print(img_url)\n",
    "    \n",
    "    else: \n",
    "        img_url = None\n",
    "        \n",
    "    return img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_object_id_from_image_url(img_url):\n",
    "    \n",
    "    ptrn = \"(objectId=[0-9]+){1}\"    \n",
    "    match = re.search(ptrn, img_url)\n",
    "    \n",
    "    if match:\n",
    "        object_id = match[0].split('=')[-1]\n",
    "    else:\n",
    "        object_id = None\n",
    "        \n",
    "    return object_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_title(soup_el):\n",
    "    \"\"\"\n",
    "    find title\n",
    "    accepts the html for a single gallery element\n",
    "    \"\"\"\n",
    "\n",
    "    soup_el_title = soup_el.find(name='span', attrs={\"class\":\"galHeadline\"})\n",
    "    \n",
    "    if soup_el_title:\n",
    "        title = soup_el_title.text\n",
    "#         print(title)\n",
    "    \n",
    "    else:\n",
    "        title = None\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_description(soup_el):\n",
    "    \"\"\"\n",
    "    find title\n",
    "    accepts the html for a single gallery element\n",
    "    \"\"\"\n",
    "    soup_el_detail = soup_el.find(name='p', attrs = {\"class\":\"galleryElementDetail\"})\n",
    "\n",
    "    if soup_el_detail:\n",
    "        detail = soup_el_detail.text.strip()\n",
    "        if len(detail) > 0:\n",
    "            return detail\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_detail_link(soup_el):\n",
    "    \"\"\"\n",
    "    find link to detail page\n",
    "    accepts the html for a single gallery element\n",
    "    \"\"\"\n",
    "    # find link to detail page\n",
    "    soup_el_links = soup_el.find_all(name='a')\n",
    "    if len(soup_el_links)>0:    \n",
    "        soup_el_link = soup_el_links[0]\n",
    "        detail_url = soup_el_link['href']\n",
    "\n",
    "    else:\n",
    "        detail_url = None\n",
    "    \n",
    "    return detail_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_one_element_details(soup_el):\n",
    "    \"\"\"\n",
    "    return a dictionary with details of one element\n",
    "    \"\"\"\n",
    "    el_dict = {}\n",
    "    \n",
    "    el_dict['title'] = find_element_title(soup_el)\n",
    "    el_dict['img_url'] = find_img_url(soup_el)\n",
    "    el_dict['detail_url'] = find_element_detail_link(soup_el)\n",
    "    el_dict['detail_description'] = find_element_description(soup_el)\n",
    "\n",
    "    if el_dict['img_url']:\n",
    "        el_dict['object_id'] = find_object_id_from_image_url(el_dict['img_url'])\n",
    "\n",
    "    return el_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_one_page_elements(soup):\n",
    "\n",
    "    page_results = []\n",
    "    soup_el_lst = find_list_of_elements(soup)\n",
    "    \n",
    "    if len(soup_el_lst)>0:\n",
    "        print('    found elements')\n",
    "    \n",
    "    for soup_el in soup_el_lst:\n",
    "        el_dict = find_one_element_details(soup_el)\n",
    "        page_results.append(el_dict)\n",
    "\n",
    "    return pd.DataFrame(page_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def download_image(img_url, file_name = None, rep_tup = (\"superImageResolution\",\"highImageResolution\")):\n",
    "    \n",
    "    if replace_dict:\n",
    "        img_url = img_url.replace(replace_dict)\n",
    "        img_url.replace(*rep_tup)\n",
    "    \n",
    "    return img_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def download_image(filename, url):\n",
    "\n",
    "    \"\"\" requests image from given url and saves it in original quality as jpeg in RGB format\n",
    "    \n",
    "    filename: local filepath to save the image to\n",
    "    url: url to request image from\"\"\"\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        logging.info('Image %s already exists. Skipping download.' % filename)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url)\n",
    "    except:\n",
    "        logging.warning('Warning: Could not download image %s from %s' % (filename, url))\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image = Image.open(BytesIO(response.read()))\n",
    "    except:\n",
    "        logging.warning('Warning: Failed to parse image %s' % filename)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image_rgb = pil_image.convert('RGB')\n",
    "    except:\n",
    "        logging.warning('Warning: Failed to convert image %s to RGB' % filename)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image_rgb.save(filename, format='JPEG')  # , quality=95\n",
    "    except:\n",
    "        logging.warning('Warning: Failed to save image %s' % filename)\n",
    "        return\n",
    "\n",
    "        \n",
    "def download_images(file_dict, destination_folder = ''):\n",
    " \n",
    "    \"\"\"loops through a dictionary of files to download. includes logging\n",
    "    \n",
    "    file_dict: should be in the format {url: file_subpath}\n",
    "    destination folder: local path to directory to save all of the images to\n",
    "    \"\"\"\n",
    "    #start logger (start_logger() makes the destination path if necessary)\n",
    "    os.mkdirs(destination_folder)\n",
    "    \n",
    "    logging_funcs.start_logger(destination_folder, logger_fname='image_downloader.log')\n",
    " \n",
    "    total_num_images = len(file_dict)   \n",
    "    print('started download of {} images to {}'.format(total_num_images, \n",
    "                                                       destination_folder))\n",
    "    \n",
    "    #loop to download from each url\n",
    "    for i, (url, file_subpath) in enumerate(file_dict.items()):\n",
    "\n",
    "        #print intermittinent milestones to console & log\n",
    "        if i % 100 == 0:\n",
    "            percent_complete = i/total_num_images\n",
    "            logging.info('Info: {:0.0%} complete'.format(percent_complete))\n",
    "            print('currently processing image {} of {} ({:0.1%} complete)'.format(i, total_num_images, percent_complete))\n",
    "            logging.info('Info: processing {} ({} of {})'.format(file_subpath, i+1, total_num_images))\n",
    "        \n",
    "        #make subfolders in file path if doesn't exist\n",
    "\n",
    "        subfolders = dirname(file_subpath)\n",
    "        if len(subfolders) == 0:\n",
    "            subfolders = 'misc'\n",
    "        subfolders_path = os.path.join(destination_folder, subfolders)\n",
    "        \n",
    "        if not os.path.exists(subfolders_path):\n",
    "            \n",
    "            os.makedirs(r'{}'.format(subfolders_path))\n",
    "            logging.info('Info: created subfolder directory %s ' % subfolders_path)\n",
    "        \n",
    "        # download image    \n",
    "            # create full filepath\n",
    "        filepath = os.path.join(destination_folder, file_subpath)\n",
    "        download_image(filepath, url)\n",
    "\n",
    "    logging.shutdown()\n",
    "    print('finished download')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_page_html(raw_html, csv_path, request_counter):\n",
    "\n",
    "    soup = BeautifulSoup(raw_html)\n",
    "    df_page = find_one_page_elements(soup)\n",
    "    df_page['results_page'] = request_counter\n",
    "\n",
    "    include_header=False\n",
    "\n",
    "    if request_counter <= 1:\n",
    "        include_header=True\n",
    "    \n",
    "    # write page\n",
    "    df_page.to_csv(fpath, mode='a', header=include_header)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_next_page(driver):\n",
    "    \n",
    "    next_button_lst = driver.find_elements_by_class_name(\"nextBtn\")\n",
    "    if next_button_lst:\n",
    "        next_button = next_button_lst[0]\n",
    "        if next_button.is_enabled():\n",
    "            next_button.click()\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place to save results\n",
    "data_dir = os.path.normpath(os.path.join(os.getcwd(), '..','data','raw','scraped'))\n",
    "fname = 'graphik_portal_results.csv'\n",
    "fpath = os.path.join(data_dir,fname)\n",
    "\n",
    "request_counter = 0\n",
    "next_clickable = True\n",
    "first_url = \"https://www.graphikportal.org/gallery/encoded/eJzjYBKS5GJLzMmJT0kVYk4tyZBidvRzUWIuycnWYhCSgUuxVZUWZSajyqpxcWfm5JQWlxQllqSmCCFzkNUBANijGqs*\"\n",
    "first_url = \"https://www.graphikportal.org/gallery/encoded/eJzjYBKS5GJLzMmJT0kVYk4tyZBidvRzUWIuycnWYhCSgUuxVZUWZSajyqpxcWfm5JQWlxQllqSmCCFzkNUBANijGqs*/5901\"\n",
    "# get first page\n",
    "driver.get(first_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current request 1\n",
      "    found elements\n",
      "current request 2\n",
      "    found elements\n",
      "current request 3\n",
      "    found elements\n",
      "current request 4\n",
      "    found elements\n",
      "current request 5\n",
      "    found elements\n",
      "current request 6\n",
      "    found elements\n",
      "current request 7\n",
      "    found elements\n",
      "current request 8\n",
      "    found elements\n",
      "current request 9\n",
      "    found elements\n",
      "current request 10\n",
      "    found elements\n",
      "current request 11\n",
      "    found elements\n",
      "current request 12\n",
      "    found elements\n",
      "current request 13\n",
      "    found elements\n",
      "current request 14\n",
      "    found elements\n",
      "current request 15\n",
      "    found elements\n",
      "current request 16\n",
      "    found elements\n",
      "current request 17\n",
      "    found elements\n",
      "current request 18\n",
      "    found elements\n",
      "current request 19\n",
      "    found elements\n",
      "current request 20\n",
      "    found elements\n",
      "current request 21\n",
      "    found elements\n",
      "current request 22\n",
      "    found elements\n",
      "current request 23\n",
      "    found elements\n",
      "current request 24\n",
      "    found elements\n",
      "current request 25\n",
      "    found elements\n",
      "current request 26\n",
      "    found elements\n",
      "current request 27\n",
      "    found elements\n",
      "current request 28\n",
      "    found elements\n",
      "current request 29\n",
      "    found elements\n",
      "current request 30\n",
      "    found elements\n",
      "current request 31\n",
      "    found elements\n",
      "current request 32\n",
      "    found elements\n",
      "current request 33\n",
      "    found elements\n",
      "current request 34\n",
      "    found elements\n",
      "current request 35\n",
      "    found elements\n",
      "current request 36\n",
      "    found elements\n",
      "current request 37\n",
      "    found elements\n",
      "current request 38\n",
      "    found elements\n",
      "current request 39\n",
      "    found elements\n",
      "current request 40\n",
      "    found elements\n",
      "current request 41\n",
      "    found elements\n",
      "current request 42\n",
      "    found elements\n",
      "current request 43\n",
      "    found elements\n",
      "current request 44\n",
      "    found elements\n",
      "current request 45\n",
      "    found elements\n",
      "current request 46\n",
      "    found elements\n",
      "current request 47\n",
      "    found elements\n",
      "current request 48\n",
      "    found elements\n",
      "current request 49\n",
      "    found elements\n",
      "current request 50\n",
      "    found elements\n",
      "current request 51\n",
      "    found elements\n",
      "current request 52\n",
      "    found elements\n",
      "current request 53\n",
      "    found elements\n",
      "current request 54\n",
      "    found elements\n",
      "current request 55\n",
      "    found elements\n"
     ]
    }
   ],
   "source": [
    "# start loop\n",
    "\n",
    "while next_clickable:\n",
    "    \n",
    "    request_counter +=1\n",
    "    print('current request {}'.format(request_counter))\n",
    "    \n",
    "    sleep(random.randint(3,5))\n",
    "\n",
    "    # save results to_csv\n",
    "    page_html = driver.page_source\n",
    "    process_one_page_html(page_html, fpath, request_counter)\n",
    "\n",
    "    # wait to not exceed throttle limits\n",
    "    sleep_time = random.randint(3,5) + (3*random.random())\n",
    "    sleep(sleep_time)\n",
    "    \n",
    "    next_clickable = navigate_to_next_page(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
