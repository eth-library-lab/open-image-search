{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse-Image Search for Graphische Sammlung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image size options   \n",
    "150x150 default  \n",
    "250x250 resolution=mediumImageResolution  \n",
    "350x350 resolution=highImageResolution  \n",
    "max resolution=superImageResolution  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url = \"https://www.e-gs.ethz.ch/eMP/eMuseumPlus?service=ImageAsset&module=collection&objectId=2562&resolution=mediumImageResolution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Downloading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_chromedriver=\"C:\\lib\\chromedriver_win32\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path_to_chromedriver)\n",
    "\n",
    "def document_initialised(driver):\n",
    "    return driver.execute_script(\"return initialised\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_url(soup):\n",
    "    \"\"\"\n",
    "    find link to next page    \n",
    "    \"\"\"\n",
    "\n",
    "#     soup_pagin = soup.find(name=\"ul\", attrs={\"class\":\"pagination\"})\n",
    "    soup_next = soup.find(name=\"a\", attrs={\"class\":\"nextBtn\"})\n",
    "    if soup_next:\n",
    "        next_url = soup_next['href']\n",
    "#         print(next_url)\n",
    "        return next_url\n",
    "    else:\n",
    "        next_url = None\n",
    "    \n",
    "    return next_url\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_list_of_elements(soup):\n",
    "\n",
    "    #find main body col-lg-12 col-md-12 list-row\n",
    "    soup_body = soup.find(name='div', attrs={\"class\":\"col-lg-12 col-md-12 list-row\"})\n",
    "    if soup_body:\n",
    "        soup_el_lst = soup_body.find_all(name='div', attrs={\"class\":\"ssy_galleryElement\"})        \n",
    "    else:\n",
    "        soup_el_lst = []\n",
    "    \n",
    "    return soup_el_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_img_url(soup_el):\n",
    "    \"\"\"\n",
    "    find link to image\n",
    "    accepts the html for a single gallery element\n",
    "    \"\"\"\n",
    "    soup_fig = soup_el.find(name='figure')\n",
    "    if soup_fig:\n",
    "        img_url = soup_fig.find(name='a').find(name='img')['src']\n",
    "#         print(img_url)\n",
    "    \n",
    "    else: \n",
    "        img_url = None\n",
    "        \n",
    "    return img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_object_id_from_image_url(img_url):\n",
    "    \n",
    "    ptrn = \"(objectId=[0-9]+){1}\"    \n",
    "    match = re.search(ptrn, img_url)\n",
    "    \n",
    "    if match:\n",
    "        object_id = match[0].split('=')[-1]\n",
    "    else:\n",
    "        object_id = None\n",
    "        \n",
    "    return object_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_title(soup_el):\n",
    "    \"\"\"\n",
    "    find title\n",
    "    accepts the html for a single gallery element\n",
    "    \"\"\"\n",
    "\n",
    "    soup_el_title = soup_el.find(name='span', attrs={\"class\":\"galHeadline\"})\n",
    "    \n",
    "    if soup_el_title:\n",
    "        title = soup_el_title.text\n",
    "#         print(title)\n",
    "    \n",
    "    else:\n",
    "        title = None\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_description(soup_el):\n",
    "    \"\"\"\n",
    "    find title\n",
    "    accepts the html for a single gallery element\n",
    "    \"\"\"\n",
    "    soup_el_detail = soup_el.find(name='p', attrs = {\"class\":\"galleryElementDetail\"})\n",
    "\n",
    "    if soup_el_detail:\n",
    "        detail = soup_el_detail.text.strip()\n",
    "        if len(detail) > 0:\n",
    "            return detail\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_element_detail_link(soup_el):\n",
    "    \"\"\"\n",
    "    find link to detail page\n",
    "    accepts the html for a single gallery element\n",
    "    \"\"\"\n",
    "    # find link to detail page\n",
    "    soup_el_links = soup_el.find_all(name='a')\n",
    "    if len(soup_el_links)>0:    \n",
    "        soup_el_link = soup_el_links[0]\n",
    "        detail_url = soup_el_link['href']\n",
    "\n",
    "    else:\n",
    "        detail_url = None\n",
    "    \n",
    "    return detail_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_one_element_details(soup_el):\n",
    "    \"\"\"\n",
    "    return a dictionary with details of one element\n",
    "    \"\"\"\n",
    "    el_dict = {}\n",
    "    \n",
    "    el_dict['title'] = find_element_title(soup_el)\n",
    "    el_dict['img_url'] = find_img_url(soup_el)\n",
    "    el_dict['detail_url'] = find_element_detail_link(soup_el)\n",
    "    el_dict['detail_description'] = find_element_description(soup_el)\n",
    "\n",
    "    if el_dict['img_url']:\n",
    "        el_dict['object_id'] = find_object_id_from_image_url(el_dict['img_url'])\n",
    "\n",
    "    return el_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_one_page_elements(soup):\n",
    "\n",
    "    page_results = []\n",
    "    soup_el_lst = find_list_of_elements(soup)\n",
    "    \n",
    "    if len(soup_el_lst)>0:\n",
    "        print('    found elements')\n",
    "    \n",
    "    for soup_el in soup_el_lst:\n",
    "        el_dict = find_one_element_details(soup_el)\n",
    "        page_results.append(el_dict)\n",
    "\n",
    "    return pd.DataFrame(page_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(filename, url):\n",
    "\n",
    "    \"\"\" requests image from given url and saves it in original quality as jpeg in RGB format\n",
    "    \n",
    "    filename: local filepath to save the image to\n",
    "    url: url to request image from\"\"\"\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print('Image %s already exists. Skipping download.' % filename)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url)\n",
    "    except:\n",
    "        logging.warning('Warning: Could not download image %s from %s' % (filename, url))\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image = Image.open(BytesIO(response.read()))\n",
    "    except:\n",
    "        print('Warning: Failed to parse image %s' % filename)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image_rgb = pil_image.convert('RGB')\n",
    "    except:\n",
    "        print('Warning: Failed to convert image %s to RGB' % filename)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        pil_image_rgb.save(filename, format='JPEG')  # , quality=95\n",
    "    except:\n",
    "        print('Warning: Failed to save image %s' % filename)\n",
    "        return\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_page_html(raw_html, csv_path, request_counter):\n",
    "\n",
    "    soup = BeautifulSoup(raw_html)\n",
    "    df_page = find_one_page_elements(soup)\n",
    "    df_page['results_page'] = request_counter\n",
    "\n",
    "    include_header=False\n",
    "\n",
    "    if request_counter <= 1:\n",
    "        include_header=True\n",
    "    \n",
    "    # write page\n",
    "    df_page.to_csv(fpath, mode='a', header=include_header)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_next_page(driver):\n",
    "    \n",
    "    next_button_lst = driver.find_elements_by_class_name(\"nextBtn\")\n",
    "    if next_button_lst:\n",
    "        next_button = next_button_lst[0]\n",
    "        if next_button.is_enabled():\n",
    "            next_button.click()\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_folder(string):\n",
    "\n",
    "#     divide into folders of 999 pictures max\n",
    "\n",
    "    if len(string)>3:\n",
    "\n",
    "        fldr = string[0:-3]\n",
    "    else:\n",
    "        fldr = \"0\"\n",
    "\n",
    "    return fldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_only_image_selenium(img_url):\n",
    "    \n",
    "    return\n",
    "\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(driver, filename, url):\n",
    "\n",
    "    \"\"\"save a screenshot as png\n",
    "    \n",
    "    filename: local filepath to save the image to\n",
    "    url: url to request image from\"\"\"\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print('    Warning: Image %s already exists. Skipping download.' % filename)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        sleep_time = random.randint(3,5) + (3*random.random())\n",
    "        sleep(sleep_time)\n",
    "        \n",
    "        img_el = driver.find_element_by_xpath('//img')\n",
    "\n",
    "\n",
    "    except:\n",
    "        print('    Warning: Could not find image element %s' % (url))\n",
    "        return\n",
    "\n",
    "    # save screenshot of image\n",
    "    try:\n",
    "        driver.get_screenshot_as_file(filename)\n",
    "#         img = Image.open(filename)\n",
    "#         img = pil_image.convert('RGB')\n",
    "    except:\n",
    "        print('    Warning: Failed to save png of image: %s' % filename)\n",
    "        return\n",
    "    \n",
    "#     try:\n",
    "#         img = center_crop_image_to_dims(img, h=h, w=w)\n",
    "#     except:\n",
    "#         print('Warning: Failed to crop image %s' % filename)\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         filename = \"\".join(filename.split('.')[:-1]) + \".jpg\"\n",
    "#         img.save(filename, format='JPEG')  # , quality=95\n",
    "#     except:\n",
    "#         print('Warning: Failed to save image as jpg %s' % filename)\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(driver, file_dict, sleep_time_range=(5,7)):\n",
    " \n",
    "    \"\"\"loops through a dictionary of files to download. includes logging\n",
    "    file_dict: should be in the format {file_fullpath:url }\n",
    "    \"\"\"\n",
    "    \n",
    "#     logging_funcs.start_logger(destination_folder, logger_fname='image_downloader.log')\n",
    " \n",
    "    total_num_images = len(file_dict)   \n",
    "    print('started download of {} images'.format(total_num_images))\n",
    "    \n",
    "    #loop to download from each url\n",
    "    for i, (filepath, url) in enumerate(file_dict.items()):\n",
    "\n",
    "        #print intermittinent milestones to console & log\n",
    "        if i % 100 == 0:\n",
    "            percent_complete = i/total_num_images\n",
    "#             logging.info('Info: {:0.0%} complete'.format(percent_complete))\n",
    "            print('currently processing image {} of {} ({:0.1%} complete)'.format(i, total_num_images, percent_complete))\n",
    "#             logging.info('Info: processing {} ({} of {})'.format(file_subpath, i+1, total_num_images))\n",
    "        \n",
    "        #make subfolders in file path if doesn't exist\n",
    "\n",
    "        subfolder = os.path.dirname(filepath)\n",
    "        \n",
    "        if not os.path.exists(subfolder):\n",
    "            \n",
    "            os.makedirs(r'{}'.format(subfolder))\n",
    "            print('    Info: created subfolder directory {}'.format(subfolder))\n",
    "        \n",
    "        # download image\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            save_image(driver, filepath, url)\n",
    "        else:\n",
    "            print('    Warning: image already exists {}'.format(filepath))\n",
    "            \n",
    "        sleep_time = random.randint(*sleep_time_range) + (random.random()*4)\n",
    "        sleep(sleep_time)\n",
    "        \n",
    "\n",
    "    print('finished download')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_search_results(driver):\n",
    "\n",
    "    # place to save results\n",
    "    data_dir = os.path.normpath(os.path.join(os.getcwd(), '..','data','raw','scraped'))\n",
    "    fname = 'graphik_portal_results.csv'\n",
    "    fpath = os.path.join(data_dir,fname)\n",
    "\n",
    "    request_counter = 0\n",
    "    next_clickable = True\n",
    "    first_url = \"https://www.graphikportal.org/gallery/encoded/eJzjYBKS5GJLzMmJT0kVYk4tyZBidvRzUWIuycnWYhCSgUuxVZUWZSajyqpxcWfm5JQWlxQllqSmCCFzkNUBANijGqs*/5901\"\n",
    "    # get first page\n",
    "    driver.get(first_url)\n",
    "\n",
    "    while next_clickable:\n",
    "\n",
    "        request_counter +=1\n",
    "        print('current request {}'.format(request_counter))\n",
    "\n",
    "        sleep(random.randint(3,5))\n",
    "\n",
    "        # save results to_csv\n",
    "        page_html = driver.page_source\n",
    "        process_one_page_html(page_html, fpath, request_counter)\n",
    "\n",
    "        # wait to not exceed throttle limits\n",
    "        sleep_time = random.randint(3,5) + (3*random.random())\n",
    "        sleep(sleep_time)\n",
    "\n",
    "        next_clickable = navigate_to_next_page(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dict_of_images_to_download(csv_fpath, data_dir):\n",
    "\n",
    "    # read in csv file\n",
    "    col_names = ['title','img_url','detail_url','detail_description','object_id', 'request_num']\n",
    "    df = pd.read_csv(csv_fpath, index_col=0, header=None)\n",
    "    df.columns = col_names \n",
    "\n",
    "    # drop any nans\n",
    "    df = df.dropna()\n",
    "\n",
    "    # make filepath column\n",
    "    fldr_path = os.path.join(data_dir, 'images')\n",
    "\n",
    "    df['filepath'] = df['object_id'].apply(set_folder)\n",
    "    df['filepath']  = fldr_path + '\\\\' + df['filepath'] + '\\\\' + df['object_id'] + \".png\"\n",
    "\n",
    "    # drop already downloaded records\n",
    "\n",
    "    # get flist of existing files   \n",
    "    existing_flist = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(fldr_path):\n",
    "        for fname in filenames:\n",
    "            cur_fpath = os.path.join(dirpath,fname)\n",
    "            existing_flist.append(cur_fpath)\n",
    "\n",
    "    orig_len = df.shape[0]\n",
    "\n",
    "    # filter out already existing\n",
    "    fltr = ~df['filepath'].isin(existing_flist)\n",
    "    df = df.loc[fltr, :]\n",
    "\n",
    "    num_dropped = orig_len - df.shape[0]\n",
    "    print('removed {} records; already saved'.format(num_dropped))\n",
    "\n",
    "    # change image url to lower res\n",
    "    df['img_url'] = df['img_url'].str.replace(\"superImageResolution\",\"highImageResolution\")\n",
    "\n",
    "    # make dict of filename:url\n",
    "    img_url_lst = df['img_url'].to_list()\n",
    "    fpath_lst = df['filepath'].to_list()\n",
    "    img_dict = dict(zip( fpath_lst, img_url_lst))\n",
    "\n",
    "return img_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.normpath(os.path.join(os.getcwd(), '..','data','raw','scraped'))\n",
    "fname = 'graphik_portal_results.csv'\n",
    "fpath = os.path.join(data_dir,fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dict = prep_dict_of_images_to_download(csv_fpath, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started download of 1548 images\n",
      "currently processing image 0 of 1548 (0.0% complete)\n",
      "    Warning: Could not find image element img_url\n",
      "currently processing image 100 of 1548 (6.5% complete)\n",
      "currently processing image 200 of 1548 (12.9% complete)\n",
      "currently processing image 300 of 1548 (19.4% complete)\n",
      "currently processing image 400 of 1548 (25.8% complete)\n",
      "currently processing image 500 of 1548 (32.3% complete)\n",
      "currently processing image 600 of 1548 (38.8% complete)\n",
      "currently processing image 700 of 1548 (45.2% complete)\n",
      "currently processing image 800 of 1548 (51.7% complete)\n",
      "currently processing image 900 of 1548 (58.1% complete)\n",
      "currently processing image 1000 of 1548 (64.6% complete)\n",
      "currently processing image 1100 of 1548 (71.1% complete)\n",
      "currently processing image 1200 of 1548 (77.5% complete)\n",
      "currently processing image 1300 of 1548 (84.0% complete)\n",
      "currently processing image 1400 of 1548 (90.4% complete)\n",
      "currently processing image 1500 of 1548 (96.9% complete)\n",
      "finished download\n"
     ]
    }
   ],
   "source": [
    "save_images(driver, img_dict, sleep_time_range=(1,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
